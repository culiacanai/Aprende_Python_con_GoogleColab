{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "295f7c67",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/culiacanai/Aprende_Python_con_GoogleColab/blob/main/notebooks/09_Web_Scraping_Basico.ipynb\" target=\"_parent\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "# üï∏Ô∏è Web Scraping B√°sico\n",
    "\n",
    "### Aprende Python con Google Colab ‚Äî por [Culiacan.AI](https://culiacan.ai)\n",
    "\n",
    "**Nivel:** üü° Intermedio  \n",
    "**Duraci√≥n estimada:** 60 minutos  \n",
    "**Requisitos:** Haber completado el [Notebook 07 ‚Äî Pandas B√°sico](07_Pandas_Basico.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "En este notebook vas a:\n",
    "- Entender qu√© es web scraping y cu√°ndo usarlo (y cu√°ndo no)\n",
    "- Hacer peticiones HTTP con la librer√≠a `requests`\n",
    "- Parsear HTML con `BeautifulSoup`\n",
    "- Extraer texto, enlaces, tablas e im√°genes de p√°ginas web\n",
    "- Limpiar y estructurar datos extra√≠dos con Pandas\n",
    "- Conocer las buenas pr√°cticas y consideraciones √©ticas\n",
    "\n",
    "> üí° **Web scraping** es el proceso de extraer datos de p√°ginas web de forma autom√°tica. Es una habilidad esencial para obtener datos que no est√°n disponibles en APIs o archivos descargables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20640a61",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 0. Preparaci√≥n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbb46c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar/importar librer√≠as\n",
    "!pip install requests beautifulsoup4 lxml -q\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "print(\"‚úÖ Librer√≠as listas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af54196",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. ¬øQu√© es web scraping?\n",
    "\n",
    "Imagina que quieres una tabla de datos de una p√°gina web. Podr√≠as copiar y pegar manualmente... o podr√≠as escribir un programa que lo haga por ti en segundos.\n",
    "\n",
    "### El proceso b√°sico:\n",
    "\n",
    "```\n",
    "1. Enviar petici√≥n HTTP ‚Üí Obtener el HTML de la p√°gina\n",
    "2. Parsear el HTML ‚Üí Convertirlo en una estructura navegable\n",
    "3. Extraer datos ‚Üí Buscar los elementos que necesitas\n",
    "4. Limpiar y guardar ‚Üí Estructurar los datos en CSV, Excel, etc.\n",
    "```\n",
    "\n",
    "### ‚ö†Ô∏è Consideraciones √©ticas y legales\n",
    "\n",
    "Antes de hacer scraping, siempre verifica:\n",
    "\n",
    "| ‚úÖ Est√° bien | ‚ùå Evita |\n",
    "|-------------|----------|\n",
    "| Datos p√∫blicos para uso personal/educativo | Datos protegidos por login |\n",
    "| Respetar el archivo `robots.txt` | Hacer miles de peticiones r√°pidas |\n",
    "| Citar la fuente de los datos | Revender datos extra√≠dos |\n",
    "| Usar APIs cuando est√©n disponibles | Ignorar los t√©rminos de servicio |\n",
    "\n",
    "> üí° **Regla de oro:** Si el sitio tiene una API, √∫sala en vez de hacer scraping. Es m√°s confiable y respetuoso.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d271037",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Hacer peticiones HTTP con requests\n",
    "\n",
    "`requests` es la librer√≠a m√°s popular de Python para hacer peticiones web.\n",
    "\n",
    "### 2.1 Tu primera petici√≥n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e386722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer una petici√≥n GET a una p√°gina\n",
    "url = \"https://quotes.toscrape.com/\"\n",
    "respuesta = requests.get(url)\n",
    "\n",
    "# Informaci√≥n de la respuesta\n",
    "print(f\"URL: {url}\")\n",
    "print(f\"C√≥digo de estado: {respuesta.status_code}\")  # 200 = √©xito\n",
    "print(f\"Tipo de contenido: {respuesta.headers.get('Content-Type', 'N/A')}\")\n",
    "print(f\"Tama√±o: {len(respuesta.text):,} caracteres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2737ea",
   "metadata": {},
   "source": [
    "### C√≥digos de estado HTTP\n",
    "\n",
    "| C√≥digo | Significado |\n",
    "|--------|------------|\n",
    "| **200** | ‚úÖ √âxito ‚Äî la p√°gina se carg√≥ bien |\n",
    "| **301/302** | ‚Ü©Ô∏è Redirecci√≥n ‚Äî la p√°gina se movi√≥ |\n",
    "| **403** | üö´ Prohibido ‚Äî no tienes acceso |\n",
    "| **404** | ‚ùå No encontrado ‚Äî la p√°gina no existe |\n",
    "| **429** | ‚è±Ô∏è Demasiadas peticiones ‚Äî te bloquearon temporalmente |\n",
    "| **500** | üí• Error del servidor |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ccfa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver el HTML crudo (primeros 500 caracteres)\n",
    "print(respuesta.text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d53161f",
   "metadata": {},
   "source": [
    "### 2.2 Headers: identificarte como navegador\n",
    "\n",
    "Algunos sitios bloquean peticiones que no parecen venir de un navegador real. Podemos enviar headers para identificarnos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ff7c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buena pr√°ctica: enviar un User-Agent\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "respuesta = requests.get(\"https://quotes.toscrape.com/\", headers=headers)\n",
    "print(f\"Status: {respuesta.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3eb210",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Parsear HTML con BeautifulSoup\n",
    "\n",
    "BeautifulSoup convierte el HTML en una estructura que puedes navegar f√°cilmente.\n",
    "\n",
    "### 3.1 Crear el soup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4ccf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir HTML a BeautifulSoup\n",
    "url = \"https://quotes.toscrape.com/\"\n",
    "respuesta = requests.get(url)\n",
    "soup = BeautifulSoup(respuesta.text, \"lxml\")\n",
    "\n",
    "# El t√≠tulo de la p√°gina\n",
    "print(f\"T√≠tulo: {soup.title.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f447cb0b",
   "metadata": {},
   "source": [
    "### 3.2 Buscar elementos\n",
    "\n",
    "BeautifulSoup tiene dos m√©todos principales para buscar:\n",
    "\n",
    "| M√©todo | Qu√© devuelve |\n",
    "|--------|-------------|\n",
    "| `find()` | El **primer** elemento que coincida |\n",
    "| `find_all()` | **Todos** los elementos que coincidan (lista) |\n",
    "\n",
    "Puedes buscar por:\n",
    "- **Etiqueta:** `soup.find(\"h1\")` ‚Äî busca `<h1>`\n",
    "- **Clase CSS:** `soup.find(\"div\", class_=\"quote\")` ‚Äî busca `<div class=\"quote\">`\n",
    "- **ID:** `soup.find(\"div\", id=\"header\")` ‚Äî busca `<div id=\"header\">`\n",
    "- **Selector CSS:** `soup.select(\"div.quote span.text\")` ‚Äî m√°s flexible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ab46bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontrar el primer elemento de un tipo\n",
    "primer_h1 = soup.find(\"h1\")\n",
    "print(f\"Primer H1: {primer_h1.text}\")\n",
    "\n",
    "# Encontrar por clase CSS\n",
    "primera_cita = soup.find(\"div\", class_=\"quote\")\n",
    "print(f\"\\nPrimera cita completa (HTML):\")\n",
    "print(primera_cita.prettify()[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35d3338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontrar TODOS los elementos\n",
    "todas_citas = soup.find_all(\"div\", class_=\"quote\")\n",
    "print(f\"Total de citas encontradas: {len(todas_citas)}\")\n",
    "\n",
    "# Extraer texto de la primera cita\n",
    "cita = todas_citas[0]\n",
    "texto = cita.find(\"span\", class_=\"text\").text\n",
    "autor = cita.find(\"small\", class_=\"author\").text\n",
    "tags = [tag.text for tag in cita.find_all(\"a\", class_=\"tag\")]\n",
    "\n",
    "print(f\"\\nTexto: {texto}\")\n",
    "print(f\"Autor: {autor}\")\n",
    "print(f\"Tags: {tags}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5c0a4a",
   "metadata": {},
   "source": [
    "### 3.3 Extraer datos estructurados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f160a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer TODAS las citas de la primera p√°gina\n",
    "url = \"https://quotes.toscrape.com/\"\n",
    "respuesta = requests.get(url)\n",
    "soup = BeautifulSoup(respuesta.text, \"lxml\")\n",
    "\n",
    "citas = []\n",
    "for div in soup.find_all(\"div\", class_=\"quote\"):\n",
    "    texto = div.find(\"span\", class_=\"text\").text\n",
    "    autor = div.find(\"small\", class_=\"author\").text\n",
    "    tags = [tag.text for tag in div.find_all(\"a\", class_=\"tag\")]\n",
    "\n",
    "    citas.append({\n",
    "        \"texto\": texto,\n",
    "        \"autor\": autor,\n",
    "        \"tags\": \", \".join(tags),\n",
    "    })\n",
    "\n",
    "# Convertir a DataFrame\n",
    "df_citas = pd.DataFrame(citas)\n",
    "print(f\"Citas extra√≠das: {len(df_citas)}\")\n",
    "df_citas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7f266a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Navegaci√≥n avanzada y selectores CSS\n",
    "\n",
    "### 4.1 Selectores CSS con select()\n",
    "\n",
    "`select()` usa la misma sintaxis que CSS ‚Äî si sabes CSS, es muy poderoso:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db60e052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selectores CSS\n",
    "url = \"https://quotes.toscrape.com/\"\n",
    "respuesta = requests.get(url)\n",
    "soup = BeautifulSoup(respuesta.text, \"lxml\")\n",
    "\n",
    "# Selector por clase\n",
    "textos = soup.select(\"span.text\")\n",
    "print(f\"Citas encontradas con select: {len(textos)}\")\n",
    "print(f\"Primera: {textos[0].text[:60]}...\")\n",
    "\n",
    "# Selector anidado: links dentro de divs con clase quote\n",
    "tags_links = soup.select(\"div.quote a.tag\")\n",
    "print(f\"\\nTags encontrados: {len(tags_links)}\")\n",
    "tags_unicos = list(set(tag.text for tag in tags_links))\n",
    "print(f\"Tags √∫nicos: {sorted(tags_unicos)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a5c9ee",
   "metadata": {},
   "source": [
    "### 4.2 Extraer atributos (href, src, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c673ad11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer todos los links de la p√°gina\n",
    "links = soup.find_all(\"a\")\n",
    "print(f\"Links encontrados: {len(links)}\\n\")\n",
    "\n",
    "for link in links[:10]:\n",
    "    texto = link.text.strip()\n",
    "    href = link.get(\"href\", \"Sin href\")\n",
    "    if texto:\n",
    "        print(f\"  [{texto}] ‚Üí {href}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f367e0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Scraping de m√∫ltiples p√°ginas (paginaci√≥n)\n",
    "\n",
    "La mayor√≠a de sitios dividen sus datos en varias p√°ginas. Necesitamos navegar entre ellas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d479a44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer citas de TODAS las p√°ginas\n",
    "todas_citas = []\n",
    "pagina = 1\n",
    "\n",
    "while True:\n",
    "    url = f\"https://quotes.toscrape.com/page/{pagina}/\"\n",
    "    respuesta = requests.get(url)\n",
    "    soup = BeautifulSoup(respuesta.text, \"lxml\")\n",
    "\n",
    "    citas_pagina = soup.find_all(\"div\", class_=\"quote\")\n",
    "\n",
    "    # Si no hay citas, terminamos\n",
    "    if not citas_pagina:\n",
    "        break\n",
    "\n",
    "    for div in citas_pagina:\n",
    "        texto = div.find(\"span\", class_=\"text\").text\n",
    "        autor = div.find(\"small\", class_=\"author\").text\n",
    "        tags = [tag.text for tag in div.find_all(\"a\", class_=\"tag\")]\n",
    "\n",
    "        todas_citas.append({\n",
    "            \"texto\": texto,\n",
    "            \"autor\": autor,\n",
    "            \"tags\": \", \".join(tags),\n",
    "            \"pagina\": pagina,\n",
    "        })\n",
    "\n",
    "    print(f\"  P√°gina {pagina}: {len(citas_pagina)} citas extra√≠das\")\n",
    "    pagina += 1\n",
    "\n",
    "    # ‚ö†Ô∏è IMPORTANTE: esperar entre peticiones para no sobrecargar el servidor\n",
    "    time.sleep(1)\n",
    "\n",
    "df_todas = pd.DataFrame(todas_citas)\n",
    "print(f\"\\n‚úÖ Total: {len(df_todas)} citas de {pagina - 1} p√°ginas\")\n",
    "df_todas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca57caa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis r√°pido de las citas\n",
    "print(f\"Total de citas: {len(df_todas)}\")\n",
    "print(f\"Autores √∫nicos: {df_todas['autor'].nunique()}\")\n",
    "\n",
    "print(f\"\\nüìä Top 5 autores con m√°s citas:\")\n",
    "print(df_todas[\"autor\"].value_counts().head())\n",
    "\n",
    "# Todos los tags\n",
    "todos_tags = []\n",
    "for tags_str in df_todas[\"tags\"]:\n",
    "    todos_tags.extend(tags_str.split(\", \"))\n",
    "\n",
    "tags_series = pd.Series(todos_tags)\n",
    "print(f\"\\nüè∑Ô∏è Top 10 tags:\")\n",
    "print(tags_series.value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04d5261",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Extraer tablas HTML\n",
    "\n",
    "Las tablas HTML son de lo m√°s com√∫n que querr√°s extraer. Pandas tiene un atajo incre√≠ble para esto.\n",
    "\n",
    "### 6.1 Con Pandas (la forma f√°cil)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723be9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas puede leer tablas HTML directamente ‚Äî ¬°una l√≠nea!\n",
    "url = \"https://www.worldometers.info/world-population/population-by-country/\"\n",
    "\n",
    "# read_html devuelve una LISTA de todas las tablas de la p√°gina\n",
    "tablas = pd.read_html(url)\n",
    "print(f\"Tablas encontradas: {len(tablas)}\")\n",
    "\n",
    "# La primera tabla suele ser la principal\n",
    "df_paises = tablas[0]\n",
    "print(f\"\\nColumnas: {list(df_paises.columns)}\")\n",
    "print(f\"Filas: {len(df_paises)}\")\n",
    "df_paises.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c816195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar: pa√≠ses de Am√©rica Latina (algunos ejemplos)\n",
    "latam = [\"Mexico\", \"Brazil\", \"Argentina\", \"Colombia\", \"Chile\", \"Peru\",\n",
    "         \"Ecuador\", \"Guatemala\", \"Cuba\", \"Bolivia\", \"Honduras\",\n",
    "         \"Paraguay\", \"El Salvador\", \"Nicaragua\", \"Costa Rica\", \"Panama\", \"Uruguay\"]\n",
    "\n",
    "# Buscar la columna que contiene el nombre del pa√≠s\n",
    "col_pais = df_paises.columns[1]  # Generalmente la segunda columna\n",
    "col_poblacion = df_paises.columns[2]  # Generalmente la tercera\n",
    "\n",
    "df_latam = df_paises[df_paises[col_pais].isin(latam)].copy()\n",
    "print(f\"Pa√≠ses de LATAM encontrados: {len(df_latam)}\")\n",
    "df_latam[[col_pais, col_poblacion]].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a852dee3",
   "metadata": {},
   "source": [
    "### 6.2 Con BeautifulSoup (m√°s control)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644fc812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer tabla manualmente con BeautifulSoup\n",
    "url = \"https://quotes.toscrape.com/tableful/\"\n",
    "\n",
    "try:\n",
    "    respuesta = requests.get(url, timeout=5)\n",
    "    soup = BeautifulSoup(respuesta.text, \"lxml\")\n",
    "\n",
    "    # Buscar la tabla\n",
    "    tabla = soup.find(\"table\")\n",
    "    if tabla:\n",
    "        # Extraer encabezados\n",
    "        encabezados = [th.text.strip() for th in tabla.find_all(\"th\")]\n",
    "        print(f\"Encabezados: {encabezados}\")\n",
    "\n",
    "        # Extraer filas\n",
    "        filas = []\n",
    "        for tr in tabla.find_all(\"tr\")[1:]:  # Saltar el encabezado\n",
    "            fila = [td.text.strip() for td in tr.find_all(\"td\")]\n",
    "            if fila:\n",
    "                filas.append(fila)\n",
    "\n",
    "        df_tabla = pd.DataFrame(filas, columns=encabezados if encabezados else None)\n",
    "        print(f\"Filas extra√≠das: {len(df_tabla)}\")\n",
    "        print(df_tabla.head())\n",
    "    else:\n",
    "        print(\"No se encontr√≥ tabla en esta p√°gina\")\n",
    "        print(\"(Esto es normal ‚Äî no todas las p√°ginas tienen tablas)\")\n",
    "except Exception as e:\n",
    "    print(f\"Nota: {e}\")\n",
    "    print(\"Esto es esperado si la URL no tiene tabla. Usemos el ejemplo de worldometers arriba.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab49701a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Ejemplo pr√°ctico: Extraer datos de libros\n",
    "\n",
    "Vamos a hacer scraping de un sitio dise√±ado para practicar: [books.toscrape.com](http://books.toscrape.com/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474ce812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer informaci√≥n de libros\n",
    "url = \"http://books.toscrape.com/\"\n",
    "respuesta = requests.get(url)\n",
    "soup = BeautifulSoup(respuesta.text, \"lxml\")\n",
    "\n",
    "# Cada libro est√° en un <article class=\"product_pod\">\n",
    "libros = []\n",
    "for articulo in soup.find_all(\"article\", class_=\"product_pod\"):\n",
    "    # T√≠tulo\n",
    "    titulo = articulo.find(\"h3\").find(\"a\")[\"title\"]\n",
    "\n",
    "    # Precio\n",
    "    precio_texto = articulo.find(\"p\", class_=\"price_color\").text\n",
    "    precio = float(precio_texto.replace(\"¬£\", \"\").replace(\"√Ç\", \"\"))\n",
    "\n",
    "    # Rating (est√° como clase CSS: \"star-rating Three\" etc.)\n",
    "    rating_map = {\"One\": 1, \"Two\": 2, \"Three\": 3, \"Four\": 4, \"Five\": 5}\n",
    "    rating_class = articulo.find(\"p\", class_=\"star-rating\")[\"class\"][1]\n",
    "    rating = rating_map.get(rating_class, 0)\n",
    "\n",
    "    # Disponibilidad\n",
    "    disponible = \"In stock\" in articulo.find(\"p\", class_=\"instock\").text\n",
    "\n",
    "    # URL del libro\n",
    "    link = articulo.find(\"h3\").find(\"a\")[\"href\"]\n",
    "\n",
    "    libros.append({\n",
    "        \"titulo\": titulo,\n",
    "        \"precio_gbp\": precio,\n",
    "        \"rating\": rating,\n",
    "        \"disponible\": disponible,\n",
    "        \"url\": f\"http://books.toscrape.com/{link}\",\n",
    "    })\n",
    "\n",
    "df_libros = pd.DataFrame(libros)\n",
    "print(f\"‚úÖ {len(df_libros)} libros extra√≠dos de la primera p√°gina\")\n",
    "df_libros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19b9656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis r√°pido\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Distribuci√≥n de precios\n",
    "axes[0].hist(df_libros[\"precio_gbp\"], bins=10, color=\"#2E86AB\", edgecolor=\"white\")\n",
    "axes[0].set_title(\"Distribuci√≥n de Precios\", fontweight=\"bold\")\n",
    "axes[0].set_xlabel(\"Precio (¬£)\")\n",
    "\n",
    "# Distribuci√≥n de ratings\n",
    "df_libros[\"rating\"].value_counts().sort_index().plot(kind=\"bar\", ax=axes[1], color=\"#A23B72\")\n",
    "axes[1].set_title(\"Distribuci√≥n de Ratings\", fontweight=\"bold\")\n",
    "axes[1].set_xlabel(\"Estrellas\")\n",
    "axes[1].set_ylabel(\"Cantidad\")\n",
    "\n",
    "# Precio promedio por rating\n",
    "df_libros.groupby(\"rating\")[\"precio_gbp\"].mean().plot(kind=\"bar\", ax=axes[2], color=\"#F18F01\")\n",
    "axes[2].set_title(\"Precio Promedio por Rating\", fontweight=\"bold\")\n",
    "axes[2].set_xlabel(\"Estrellas\")\n",
    "axes[2].set_ylabel(\"Precio (¬£)\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Precio promedio: ¬£{df_libros['precio_gbp'].mean():.2f}\")\n",
    "print(f\"Libro m√°s caro: {df_libros.loc[df_libros['precio_gbp'].idxmax(), 'titulo']} (¬£{df_libros['precio_gbp'].max()})\")\n",
    "print(f\"Libro m√°s barato: {df_libros.loc[df_libros['precio_gbp'].idxmin(), 'titulo']} (¬£{df_libros['precio_gbp'].min()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6b3995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer libros de las primeras 5 p√°ginas\n",
    "todos_libros = []\n",
    "\n",
    "for pagina in range(1, 6):  # P√°ginas 1 a 5\n",
    "    url = f\"http://books.toscrape.com/catalogue/page-{pagina}.html\"\n",
    "    respuesta = requests.get(url)\n",
    "\n",
    "    if respuesta.status_code != 200:\n",
    "        print(f\"  P√°gina {pagina}: Error {respuesta.status_code}\")\n",
    "        break\n",
    "\n",
    "    soup = BeautifulSoup(respuesta.text, \"lxml\")\n",
    "    rating_map = {\"One\": 1, \"Two\": 2, \"Three\": 3, \"Four\": 4, \"Five\": 5}\n",
    "\n",
    "    for art in soup.find_all(\"article\", class_=\"product_pod\"):\n",
    "        titulo = art.find(\"h3\").find(\"a\")[\"title\"]\n",
    "        precio = float(art.find(\"p\", class_=\"price_color\").text.replace(\"¬£\", \"\").replace(\"√Ç\", \"\"))\n",
    "        rating = rating_map.get(art.find(\"p\", class_=\"star-rating\")[\"class\"][1], 0)\n",
    "\n",
    "        todos_libros.append({\n",
    "            \"titulo\": titulo,\n",
    "            \"precio_gbp\": precio,\n",
    "            \"rating\": rating,\n",
    "            \"pagina\": pagina,\n",
    "        })\n",
    "\n",
    "    print(f\"  P√°gina {pagina}: {len(soup.find_all('article', class_='product_pod'))} libros\")\n",
    "    time.sleep(0.5)  # Respetar el servidor\n",
    "\n",
    "df_todos = pd.DataFrame(todos_libros)\n",
    "print(f\"\\n‚úÖ Total: {len(df_todos)} libros extra√≠dos\")\n",
    "print(f\"Precio promedio: ¬£{df_todos['precio_gbp'].mean():.2f}\")\n",
    "print(f\"Rating promedio: {df_todos['rating'].mean():.1f} estrellas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95844fe4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Buenas pr√°cticas\n",
    "\n",
    "### 8.1 Revisar robots.txt\n",
    "\n",
    "Siempre revisa el archivo `robots.txt` de un sitio antes de hacer scraping:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e62cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar robots.txt\n",
    "url = \"http://books.toscrape.com/robots.txt\"\n",
    "respuesta = requests.get(url)\n",
    "\n",
    "if respuesta.status_code == 200:\n",
    "    print(f\"robots.txt de books.toscrape.com:\")\n",
    "    print(respuesta.text)\n",
    "else:\n",
    "    print(f\"No hay robots.txt (status: {respuesta.status_code})\")\n",
    "    print(\"Esto significa que no hay restricciones expl√≠citas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952bbff3",
   "metadata": {},
   "source": [
    "### 8.2 Funci√≥n de scraping robusta\n",
    "\n",
    "Una funci√≥n que maneja errores, reintentos y l√≠mites:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27059dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_pagina(url: str, max_reintentos: int = 3, espera: float = 1.0) -> BeautifulSoup | None:\n",
    "    \"\"\"\n",
    "    Descarga y parsea una p√°gina web de forma robusta.\n",
    "\n",
    "    Args:\n",
    "        url: URL de la p√°gina a descargar.\n",
    "        max_reintentos: N√∫mero m√°ximo de intentos si falla.\n",
    "        espera: Segundos a esperar entre reintentos.\n",
    "\n",
    "    Returns:\n",
    "        Objeto BeautifulSoup o None si fall√≥.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (compatible; PythonBot/1.0; +https://culiacan.ai)\"\n",
    "    }\n",
    "\n",
    "    for intento in range(max_reintentos):\n",
    "        try:\n",
    "            respuesta = requests.get(url, headers=headers, timeout=10)\n",
    "\n",
    "            if respuesta.status_code == 200:\n",
    "                return BeautifulSoup(respuesta.text, \"lxml\")\n",
    "            elif respuesta.status_code == 429:\n",
    "                print(f\"‚è±Ô∏è Demasiadas peticiones, esperando {espera * (intento + 1)}s...\")\n",
    "                time.sleep(espera * (intento + 1))\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Status {respuesta.status_code} para {url}\")\n",
    "                return None\n",
    "\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"‚è±Ô∏è Timeout en intento {intento + 1}\")\n",
    "            time.sleep(espera)\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(f\"üîå Error de conexi√≥n en intento {intento + 1}\")\n",
    "            time.sleep(espera)\n",
    "\n",
    "    print(f\"‚ùå No se pudo acceder a {url} despu√©s de {max_reintentos} intentos\")\n",
    "    return None\n",
    "\n",
    "# Probar la funci√≥n\n",
    "soup = obtener_pagina(\"https://quotes.toscrape.com/\")\n",
    "if soup:\n",
    "    print(f\"‚úÖ P√°gina descargada: {soup.title.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963d36f1",
   "metadata": {},
   "source": [
    "### 8.3 Checklist de buenas pr√°cticas\n",
    "\n",
    "Antes de hacer scraping, siempre:\n",
    "\n",
    "1. ‚úÖ **Revisa si hay una API** ‚Äî es mejor que scraping\n",
    "2. ‚úÖ **Lee robots.txt** ‚Äî `sitio.com/robots.txt`\n",
    "3. ‚úÖ **Lee los t√©rminos de servicio** del sitio\n",
    "4. ‚úÖ **Agrega pausas** entre peticiones (`time.sleep()`)\n",
    "5. ‚úÖ **Usa User-Agent** apropiado\n",
    "6. ‚úÖ **Maneja errores** con try/except\n",
    "7. ‚úÖ **No sobrecargues** el servidor (m√°ximo 1 petici√≥n por segundo)\n",
    "8. ‚úÖ **Guarda los datos** para no repetir el scraping innecesariamente\n",
    "9. ‚úÖ **Cita la fuente** cuando uses los datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5c5587",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Guardar resultados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b10613d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"datos\", exist_ok=True)\n",
    "\n",
    "# Guardar citas\n",
    "df_todas.to_csv(\"datos/citas_scraping.csv\", index=False)\n",
    "print(f\"‚úÖ citas_scraping.csv: {len(df_todas)} citas\")\n",
    "\n",
    "# Guardar libros\n",
    "df_todos.to_csv(\"datos/libros_scraping.csv\", index=False)\n",
    "print(f\"‚úÖ libros_scraping.csv: {len(df_todos)} libros\")\n",
    "\n",
    "# Tambi√©n en Excel\n",
    "with pd.ExcelWriter(\"datos/scraping_resultados.xlsx\") as writer:\n",
    "    df_todas.to_excel(writer, sheet_name=\"Citas\", index=False)\n",
    "    df_todos.to_excel(writer, sheet_name=\"Libros\", index=False)\n",
    "\n",
    "print(\"‚úÖ scraping_resultados.xlsx (2 hojas)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ddf886",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. üèÜ Mini Proyecto: Scraper de noticias de Wikipedia\n",
    "\n",
    "Vamos a extraer la tabla de los pa√≠ses m√°s poblados del mundo desde Wikipedia:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ebb251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üèÜ Mini Proyecto: Datos de pa√≠ses desde Wikipedia\n",
    "\n",
    "# Wikipedia permite scraping y sus tablas son f√°ciles de extraer con Pandas\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population\"\n",
    "\n",
    "print(\"üì• Descargando datos de Wikipedia...\")\n",
    "tablas = pd.read_html(url)\n",
    "print(f\"Tablas encontradas: {len(tablas)}\")\n",
    "\n",
    "# La tabla principal suele ser la m√°s grande\n",
    "df = max(tablas, key=len)\n",
    "print(f\"Tabla principal: {len(df)} filas, {len(df.columns)} columnas\")\n",
    "print(f\"Columnas: {list(df.columns)}\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497c7bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar y analizar\n",
    "# Nota: las columnas pueden variar, ajustamos seg√∫n lo que encontremos\n",
    "\n",
    "# Renombrar columnas para facilitar el trabajo\n",
    "df_clean = df.copy()\n",
    "df_clean.columns = [str(c).strip() for c in df_clean.columns]\n",
    "\n",
    "print(\"Columnas disponibles:\")\n",
    "for i, col in enumerate(df_clean.columns):\n",
    "    print(f\"  [{i}] {col}\")\n",
    "\n",
    "# Mostrar las primeras filas para entender la estructura\n",
    "print(\"\\nPrimeras filas:\")\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e36c29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n r√°pida: Top 15 pa√≠ses m√°s poblados\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Intentar identificar las columnas correctas\n",
    "# Generalmente: columna 1 = pa√≠s, columna 2 = poblaci√≥n\n",
    "col_pais = df_clean.columns[1]\n",
    "col_poblacion = df_clean.columns[2]\n",
    "\n",
    "# Tomar top 15\n",
    "top15 = df_clean.head(15).copy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Limpiar datos de poblaci√≥n (quitar comas, convertir a n√∫mero)\n",
    "try:\n",
    "    top15[col_poblacion] = pd.to_numeric(\n",
    "        top15[col_poblacion].astype(str).str.replace(\",\", \"\").str.replace(\" \", \"\"),\n",
    "        errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    top15_sorted = top15.sort_values(col_poblacion, ascending=True)\n",
    "\n",
    "    ax.barh(top15_sorted[col_pais].astype(str), top15_sorted[col_poblacion],\n",
    "            color=\"#2E86AB\", edgecolor=\"white\")\n",
    "    ax.set_title(\"Top 15 Pa√≠ses M√°s Poblados del Mundo\", fontsize=14, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"Poblaci√≥n\")\n",
    "    ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f\"{x/1e6:.0f}M\"))\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Guardar resultado limpio\n",
    "    top15.to_csv(\"datos/top15_paises_poblacion.csv\", index=False)\n",
    "    print(\"\\n‚úÖ top15_paises_poblacion.csv guardado\")\n",
    "except Exception as e:\n",
    "    print(f\"Nota: La estructura de Wikipedia puede variar. Error: {e}\")\n",
    "    print(\"Intenta ajustar las columnas manualmente bas√°ndote en el output anterior\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c634604f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üî• Retos\n",
    "\n",
    "1. **Scraper de libros completo:** Extrae TODAS las p√°ginas (50) de books.toscrape.com. Analiza: distribuci√≥n de precios por categor√≠a, rating promedio, y los 10 libros m√°s caros.\n",
    "\n",
    "2. **Scraper de autores:** En quotes.toscrape.com, cada autor tiene una p√°gina con su biograf√≠a (`/author/nombre`). Extrae el nombre, fecha de nacimiento, lugar y descripci√≥n de los 10 primeros autores.\n",
    "\n",
    "3. **Wikipedia LATAM:** Extrae de Wikipedia la tabla de poblaci√≥n de pa√≠ses de Am√©rica Latina. Limpia los datos, calcula el total, y genera una gr√°fica de barras con el top 10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f8443e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reto 1: Scraper de libros completo\n",
    "# Tu c√≥digo aqu√≠ üëá\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24fdd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reto 2: Scraper de autores\n",
    "# Tu c√≥digo aqu√≠ üëá\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d457ec0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reto 3: Wikipedia LATAM\n",
    "# Tu c√≥digo aqu√≠ üëá\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94a6fdc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã Resumen\n",
    "\n",
    "### Proceso de web scraping\n",
    "\n",
    "| Paso | Herramienta | C√≥digo |\n",
    "|------|------------|--------|\n",
    "| 1. Descargar HTML | `requests` | `requests.get(url)` |\n",
    "| 2. Parsear HTML | `BeautifulSoup` | `BeautifulSoup(html, \"lxml\")` |\n",
    "| 3. Buscar elementos | BS4 | `soup.find()`, `soup.find_all()`, `soup.select()` |\n",
    "| 4. Extraer datos | BS4 | `.text`, `.get(\"href\")`, `[\"class\"]` |\n",
    "| 5. Tablas directo | `Pandas` | `pd.read_html(url)` |\n",
    "| 6. Guardar | `Pandas` | `df.to_csv()`, `df.to_excel()` |\n",
    "\n",
    "### M√©todos clave de BeautifulSoup\n",
    "\n",
    "| M√©todo | Ejemplo | Qu√© hace |\n",
    "|--------|---------|---------|\n",
    "| `find()` | `soup.find(\"div\", class_=\"quote\")` | Primer elemento |\n",
    "| `find_all()` | `soup.find_all(\"a\")` | Todos los elementos |\n",
    "| `select()` | `soup.select(\"div.quote span.text\")` | Selector CSS |\n",
    "| `.text` | `elemento.text` | Texto del elemento |\n",
    "| `.get()` | `link.get(\"href\")` | Valor de un atributo |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚è≠Ô∏è ¬øQu√© sigue?\n",
    "\n",
    "En el siguiente notebook aprender√°s sobre **APIs y JSON** ‚Äî c√≥mo consumir datos de servicios web de forma estructurada y profesional.\n",
    "\n",
    "üëâ [10 ‚Äî APIs y JSON](10_APIs_y_JSON.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "<p align=\"center\">\n",
    "  Hecho con ‚ù§Ô∏è por <a href=\"https://culiacan.ai\">Culiacan.AI</a> ‚Äî Culiac√°n reconocida en el mundo por su talento y emprendimiento en Inteligencia Artificial\n",
    "</p>\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
